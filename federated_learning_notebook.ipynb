{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b4eeb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 19:56:15.328461: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-20 19:56:15.643984: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-20 19:56:15.645870: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-20 19:56:16.869134: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "# Constants\n",
    "RSA_SIZE = 2048\n",
    "DEFAULT_CURVE = \"secp384r1\"\n",
    "FAST_CURVE = \"secp256r1\"\n",
    "SUPPORTED_CURVES = [DEFAULT_CURVE, FAST_CURVE]\n",
    "DIGEST_SHA384 = \"sha384\"\n",
    "DIGEST_SHA256 = \"sha256\"\n",
    "server = \"https://127.0.0.1:8000\"\n",
    "num_users = 4\n",
    "url = server + \"/app/api/metrics\"\n",
    "workspace_path = \"workspace\"\n",
    "\n",
    "# Helper function to get file path\n",
    "def get_workspace_path(file_name):\n",
    "    return os.path.join(os.getcwd(), workspace_path, file_name)\n",
    "\n",
    "# Paths to certificate and key files\n",
    "service_cert_path = get_workspace_path(\"sandbox_common/service_cert.pem\")\n",
    "user0_cert_path = get_workspace_path(\"sandbox_common/user0_cert.pem\")\n",
    "user0_privk_path = get_workspace_path(\"sandbox_common/user0_privk.pem\")\n",
    "user1_cert_path = get_workspace_path(\"sandbox_common/user1_cert.pem\")\n",
    "user1_privk_path = get_workspace_path(\"sandbox_common/user1_privk.pem\")\n",
    "member0_cert_path = get_workspace_path(\"sandbox_common/member0_cert.pem\")\n",
    "member0_privk_path = get_workspace_path(\"sandbox_common/member0_privk.pem\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1052cb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is healthy.\n",
      "Status Code: 200\n",
      "\n",
      "Response Headers:\n",
      "content-length: 347\n",
      "content-type: application/json\n",
      "\n",
      "Response Body:\n",
      "{\n",
      "    \"metrics\": [\n",
      "        {\n",
      "            \"calls\": 2,\n",
      "            \"errors\": 0,\n",
      "            \"failures\": 0,\n",
      "            \"method\": \"GET\",\n",
      "            \"path\": \"api/metrics\",\n",
      "            \"retries\": 0\n",
      "        },\n",
      "        {\n",
      "            \"calls\": 2,\n",
      "            \"errors\": 0,\n",
      "            \"failures\": 0,\n",
      "            \"method\": \"GET\",\n",
      "            \"path\": \"commit\",\n",
      "            \"retries\": 0\n",
      "        },\n",
      "        {\n",
      "            \"calls\": 1,\n",
      "            \"errors\": 0,\n",
      "            \"failures\": 0,\n",
      "            \"method\": \"POST\",\n",
      "            \"path\": \"model/intial_model\",\n",
      "            \"retries\": 0\n",
      "        },\n",
      "        {\n",
      "            \"calls\": 2,\n",
      "            \"errors\": 0,\n",
      "            \"failures\": 0,\n",
      "            \"method\": \"GET\",\n",
      "            \"path\": \"status\",\n",
      "            \"retries\": 0\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check server health\n",
    "def checkServerHealth():\n",
    "    try:\n",
    "        response = requests.get(f'{server}/app/status', verify=service_cert_path)\n",
    "        if response.status_code == 200:\n",
    "            print(\"Server is healthy.\")\n",
    "        else:\n",
    "            print(f\"Server is not healthy. Status code: {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error making request:\", e)\n",
    "\n",
    "checkServerHealth()\n",
    "\n",
    "# Get response from server\n",
    "try:\n",
    "    response = requests.get(url, verify=service_cert_path)\n",
    "\n",
    "    print(\"Status Code:\", response.status_code)\n",
    "    print(\"\\nResponse Headers:\")\n",
    "    for header, value in response.headers.items():\n",
    "        print(f\"{header}: {value}\")\n",
    "\n",
    "    print(\"\\nResponse Body:\")\n",
    "    try:\n",
    "        response_json = response.json()\n",
    "        print(json.dumps(response_json, indent=4))\n",
    "    except ValueError:\n",
    "        print(response.text)\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"Error making request:\", e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "341da4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aamar/Downloads/project/CCF_FL_Block\n",
      "total 1800\n",
      "drwxrwxr-x 3 aamar aamar    4096 Jun 20 19:48 build\n",
      "drwxrwxrwx 2 aamar aamar    4096 Jun 12 14:15 cmake\n",
      "-rwxrwxrwx 1 aamar aamar    1752 Jun 12 14:15 CMakeLists.txt\n",
      "drwxrwxrwx 2 aamar aamar    4096 Jun 12 14:15 config\n",
      "drwxrwxrwx 3 aamar aamar    4096 Jun 12 14:15 cpp\n",
      "drwxrwxrwx 2 aamar aamar    4096 Jun 12 14:15 docker\n",
      "-rwxrwxrwx 1 aamar aamar     179 Jun 12 14:15 Dockerfile\n",
      "-rwxrwxrwx 1 aamar aamar     150 Jun 12 14:15 Dockerfile.ignore\n",
      "-rwxrwxrwx 1 aamar aamar     750 Jun 12 14:15 Dockerfile.virtual\n",
      "drwxrwxrwx 2 aamar aamar    4096 Jun 20 19:32 experiments\n",
      "-rw-rw-r-- 1 aamar aamar   19649 Jun 20 19:45 federated_learning_notebook.ipynb\n",
      "-rwxrwxrwx 1 aamar aamar 1387083 Jun 12 14:15 FL_Clients.ipynb\n",
      "-rwxrwxrwx 1 aamar aamar    1161 Jun 12 14:15 LICENSE\n",
      "-rwxrwxrwx 1 aamar aamar    4440 Jun 12 14:15 Makefile\n",
      "-rwxrwxrwx 1 aamar aamar   53243 Jun 13 00:27 mnistExp.ipynb\n",
      "-rwxrwxrwx 1 aamar aamar     320 Jun 12 14:15 oe_sign.conf\n",
      "-rwxrwxrwx 1 aamar aamar   10281 Jun 12 14:15 README.md\n",
      "-rwxrwxrwx 1 aamar aamar     239 Jun 14 16:16 requirements.txt\n",
      "-rwxrwxrwx 1 aamar aamar    2703 Jun 12 14:15 SECURITY.md\n",
      "-rw-r--r-- 1 root  root   130858 Jun 20 18:52 temp_model.keras\n",
      "-rw-rw-r-- 1 aamar aamar  130957 Jun 20 19:50 temp_model.pkl\n",
      "-rwxrwxrwx 1 aamar aamar   28956 Jun 12 14:15 test.ipynb\n",
      "-rwxrwxrwx 1 aamar aamar    3061 Jun 12 14:15 test.sh\n",
      "drwxrwxr-x 4 aamar aamar    4096 Jun 20 19:48 workspace\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "!ls -l {os.getcwd()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efce324e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to compute gradients\n",
    "def compute_gradients(model, X, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(X, training=True)\n",
    "        loss = tf.keras.losses.categorical_crossentropy(y, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    return gradients\n",
    "\n",
    "# Function to aggregate weights\n",
    "def aggregate_weight(client_weights_list):\n",
    "    if client_weights_list:\n",
    "        total_weights = sum(client_weights_list, [])\n",
    "        aggregated_weights = [weight / len(client_weights_list) for weight in total_weights]\n",
    "        return aggregated_weights\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Function to create the model\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(64, kernel_size=3, activation='relu', input_shape=(28, 28, 1)),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(32, kernel_size=3, activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Function to aggregate weights\n",
    "def aggregate_weights(model_id, round_no, user_cert, user_key):\n",
    "    response = requests.put(\n",
    "        url=f\"{server}/app/model/aggregate_weights_local?model_id={model_id}&round_no={round_no}\",\n",
    "        verify=service_cert_path,\n",
    "        cert=(user_cert, user_key)\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        print(\"Aggregation successful for model:\", model_id)\n",
    "    else:\n",
    "        raise Exception(f\"Failed to aggregate weights. Status code: {response.status_code}\")\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, X_train, y_train, X_test, y_test, user_id, round_no, epochs=1):\n",
    "    batch_size = 16\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size)\n",
    "    return history.history['loss']\n",
    "\n",
    "# Function to serialize the model\n",
    "def serialize_model(model):\n",
    "    with open('temp_model.pkl', 'wb') as file:\n",
    "        joblib.dump(model, file)\n",
    "    with open('temp_model.pkl', 'rb') as file:\n",
    "        return base64.b64encode(file.read()).decode('utf-8')\n",
    "\n",
    "# Function to upload the initial model\n",
    "def upload_initial_model(model_base64, user_cert, user_key):\n",
    "    payload = {\n",
    "        \"global_model\": {\n",
    "            \"model_name\": \"CNNModel\",\n",
    "            \"model_data\": model_base64\n",
    "        }\n",
    "    }\n",
    "    response = requests.post(\n",
    "        url=f\"{server}/app/model/intial_model\",\n",
    "        verify=service_cert_path,\n",
    "        cert=(user_cert, user_key),\n",
    "        json=payload\n",
    "    )\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        model_data = response.json()\n",
    "        model_id = model_data.get(\"model_id\")\n",
    "        model_name = model_data.get(\"model_name\")\n",
    "        print(f\"Initial global model '{model_name}' (ID: {model_id}) uploaded successfully.\")\n",
    "        return model_id\n",
    "    else:\n",
    "        print(f\"Failed to upload initial model. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Function to flatten weights\n",
    "def flatten_weights(model):\n",
    "    flat_weights = []\n",
    "    for layer in model.layers:\n",
    "        weights = layer.get_weights()\n",
    "        if weights:\n",
    "            flat_weights.append(weights[0].flatten())\n",
    "    return np.concatenate(flat_weights)\n",
    "\n",
    "# Function to deserialize weights\n",
    "def deserialize_weights(serialized_weights, model):\n",
    "    flat_weights = np.array(serialized_weights)\n",
    "    unflattened_weights = unflatten_weights(model, flat_weights)\n",
    "    return unflattened_weights\n",
    "\n",
    "# Function to unflatten weights\n",
    "def unflatten_weights(model, flat_weights):\n",
    "    unflattened_weights = []\n",
    "    index = 0\n",
    "    for layer in model.layers:\n",
    "        layer_weights = layer.get_weights()\n",
    "        if layer_weights:\n",
    "            weights_shape = layer_weights[0].shape\n",
    "            layer_weights_unflattened = flat_weights[index:index + np.prod(weights_shape)].reshape(weights_shape)\n",
    "            unflattened_weights.append(layer_weights_unflattened)\n",
    "            index += np.prod(weights_shape)\n",
    "    unflattened_weights = [np.array(arr) for arr in unflattened_weights]\n",
    "    return unflattened_weights\n",
    "\n",
    "# Function to serialize gradients\n",
    "def serialize_gradients(gradients):\n",
    "    serialized_gradients = [grad.tolist() for grad in gradients]\n",
    "    return json.dumps(serialized_gradients)\n",
    "\n",
    "# Function to deserialize gradients\n",
    "def deserialize_gradients(serialized_gradients):\n",
    "    gradients_list = json.loads(serialized_gradients)\n",
    "    return [np.array(grad) for grad in gradients_list]\n",
    "\n",
    "# Function to upload gradients\n",
    "def upload_gradients(gradients_base64, user_cert, user_key, round_no, model_id=None):\n",
    "    print(f\"Uploading gradients for Round {round_no}...\")\n",
    "    payload = {\n",
    "        \"model_id\": model_id,\n",
    "        \"gradients_json\": gradients_base64,\n",
    "        \"round_no\": round_no\n",
    "    }\n",
    "    response = requests.post(\n",
    "        url=f\"{server}/app/model/upload/local_gradients\",\n",
    "        verify=service_cert_path,\n",
    "        cert=(user_cert, user_key),\n",
    "        json=payload\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Gradients uploaded successfully for Round {round_no}.\")\n",
    "    else:\n",
    "        raise Exception(f\"Failed to upload gradients. Status code: {response.status_code}\")\n",
    "\n",
    "# Function to download global gradients\n",
    "def download_global_gradients(user_cert, user_key, model_id):\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            url=f\"{server}/app/model/download_global_gradients?model_id={model_id}\",\n",
    "            verify=service_cert_path,\n",
    "            cert=(user_cert, user_key)\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            print(\"Global gradients downloaded successfully.\")\n",
    "            response_data = response.json()\n",
    "            global_gradients_value = response_data.get(\"global_gradients\")\n",
    "            if global_gradients_value:\n",
    "                gradients = deserialize_gradients(global_gradients_value)\n",
    "                return gradients\n",
    "            else:\n",
    "                print(\"Global gradients data not found in response.\")\n",
    "        else:\n",
    "            print(f\"Failed to download global gradients. Status code: {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error making request:\", e)\n",
    "    return None\n",
    "\n",
    "# Function to apply gradients\n",
    "def apply_gradients(model, gradients):\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "# Function to delete temporary model file\n",
    "def delete_temp_model_file():\n",
    "    if os.path.exists('temp_model.pkl'):\n",
    "        os.remove('temp_model.pkl')\n",
    "\n",
    "# Function to plot loss curve\n",
    "def plot_loss_curve(round_loss_dict):\n",
    "    rounds = list(round_loss_dict.keys())\n",
    "    losses_user0 = [round_loss_dict[round][0] for round in rounds]\n",
    "    losses_user1 = [round_loss_dict[round][1] for round in rounds]\n",
    "\n",
    "    plt.plot(rounds, losses_user0, label='User 0 Loss')\n",
    "    plt.plot(rounds, losses_user1, label='User 1 Loss')\n",
    "\n",
    "    plt.xlabel('Round Number')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss Curve for Each User')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97fea6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial global model 'CNNModel' (ID: 1) uploaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 19:56:19.768242: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 5191680000 exceeds 10% of free system memory.\n",
      "2024-06-20 19:56:21.248675: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 5191680000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load MNIST data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "num_users = 2\n",
    "user_split_size = X_train.shape[0] // num_users\n",
    "X_train_user0 = X_train[:user_split_size]\n",
    "y_train_user0 = y_train[:user_split_size]\n",
    "X_train_user1 = X_train[user_split_size:user_split_size*2]\n",
    "y_train_user1 = y_train[user_split_size:user_split_size*2]\n",
    "\n",
    "global_model = create_model()\n",
    "\n",
    "serialized_model = serialize_model(global_model)\n",
    "initial_model_id = upload_initial_model(serialized_model, user0_cert_path, user0_privk_path)\n",
    "\n",
    "num_rounds = 5\n",
    "round_loss_dict = {}\n",
    "\n",
    "local_model_user0 = global_model\n",
    "local_model_user1 = global_model\n",
    "\n",
    "for round_no in range(1, num_rounds + 1):\n",
    "    round_loss_dict[round_no] = {}\n",
    "    for user_id in range(2):\n",
    "        X_train_user = X_train_user0 if user_id == 0 else X_train_user1\n",
    "        y_train_user = y_train_user0 if user_id == 0 else y_train_user1\n",
    "        gradients = compute_gradients(local_model_user0 if user_id == 0 else local_model_user1, X_train_user, y_train_user)\n",
    "        serialized_gradients = serialize_gradients(gradients)\n",
    "        print(f\"User {user_id} - Round {round_no} - Gradients Length: {len(serialized_gradients)}\")\n",
    "        upload_gradients(serialized_gradients, user0_cert_path if user_id == 0 else user1_cert_path, user0_privk_path if user_id == 0 else user1_privk_path, round_no, model_id=initial_model_id)\n",
    "    \n",
    "    global_gradients = download_global_gradients(user0_cert_path, user0_privk_path, model_id=initial_model_id)\n",
    "    if global_gradients:\n",
    "        apply_gradients(local_model_user0, global_gradients)\n",
    "        apply_gradients(local_model_user1, global_gradients)\n",
    "    else:\n",
    "        print(\"No global gradients received for this round.\")\n",
    "\n",
    "print(\"Federated Learning Process Completed.\")\n",
    "delete_temp_model_file()\n",
    "plot_loss_curve(round_loss_dict)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
