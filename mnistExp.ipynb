{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MINST Dataset Experiment with FL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./.venv/lib/python3.8/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.8/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.8/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.8/site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.8/site-packages (from requests) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "\n",
      "Response Headers:\n",
      "content-length: 672\n",
      "content-type: application/json\n",
      "\n",
      "Response Body:\n",
      "{\n",
      "    \"metrics\": [\n",
      "        {\n",
      "            \"calls\": 9,\n",
      "            \"errors\": 0,\n",
      "            \"failures\": 0,\n",
      "            \"method\": \"GET\",\n",
      "            \"path\": \"api/metrics\",\n",
      "            \"retries\": 0\n",
      "        },\n",
      "        {\n",
      "            \"calls\": 2,\n",
      "            \"errors\": 0,\n",
      "            \"failures\": 0,\n",
      "            \"method\": \"GET\",\n",
      "            \"path\": \"commit\",\n",
      "            \"retries\": 0\n",
      "        },\n",
      "        {\n",
      "            \"calls\": 4,\n",
      "            \"errors\": 0,\n",
      "            \"failures\": 0,\n",
      "            \"method\": \"PUT\",\n",
      "            \"path\": \"model/aggregate_weights_local\",\n",
      "            \"retries\": 0\n",
      "        },\n",
      "        {\n",
      "            \"calls\": 8,\n",
      "            \"errors\": 0,\n",
      "            \"failures\": 0,\n",
      "            \"method\": \"GET\",\n",
      "            \"path\": \"model/download/global\",\n",
      "            \"retries\": 0\n",
      "        },\n",
      "        {\n",
      "            \"calls\": 4,\n",
      "            \"errors\": 4,\n",
      "            \"failures\": 0,\n",
      "            \"method\": \"GET\",\n",
      "            \"path\": \"model/download_gloabl_weights\",\n",
      "            \"retries\": 0\n",
      "        },\n",
      "        {\n",
      "            \"calls\": 7,\n",
      "            \"errors\": 0,\n",
      "            \"failures\": 0,\n",
      "            \"method\": \"POST\",\n",
      "            \"path\": \"model/intial_model\",\n",
      "            \"retries\": 0\n",
      "        },\n",
      "        {\n",
      "            \"calls\": 8,\n",
      "            \"errors\": 0,\n",
      "            \"failures\": 0,\n",
      "            \"method\": \"POST\",\n",
      "            \"path\": \"model/upload/local_model_weights\",\n",
      "            \"retries\": 0\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "RSA_SIZE = 2048\n",
    "\n",
    "\n",
    "DEFAULT_CURVE = \"secp384r1\"\n",
    "FAST_CURVE = \"secp256r1\"\n",
    "SUPPORTED_CURVES = [DEFAULT_CURVE, FAST_CURVE]\n",
    "\n",
    "DIGEST_SHA384 = \"sha384\"\n",
    "DIGEST_SHA256 = \"sha256\"\n",
    "\n",
    "RSA_SIZE = 2048\n",
    "# CCF network node\n",
    "server=\"https://127.0.0.1:8000\"\n",
    "\n",
    "num_users = 4\n",
    "# Getting Network metrices\n",
    "url = server + \"/app/api/metrics\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, verify='./workspace/sandbox_common/service_cert.pem')\n",
    "\n",
    "    print(\"Status Code:\", response.status_code)\n",
    "    print(\"\\nResponse Headers:\")\n",
    "    for header, value in response.headers.items():\n",
    "        print(f\"{header}: {value}\")\n",
    "\n",
    "    print(\"\\nResponse Body:\")\n",
    "    try:\n",
    "        # Attempt to parse JSON and print it in an indented format\n",
    "        response_json = response.json()\n",
    "        print(json.dumps(response_json, indent=4))\n",
    "    except ValueError:\n",
    "        # If response is not JSON, print as plain text\n",
    "        print(response.text)\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"Error making request:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 19:40:14.341605: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-19 19:40:14.406177: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-19 19:40:14.407957: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-19 19:40:15.515115: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import base64\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "server = \"https://127.0.0.1:8000\"  # Replace with your server URL\n",
    "\n",
    "\n",
    "\n",
    "def download_global_model_weights(user_cert, user_key, model_id, local_model):\n",
    "    try:\n",
    "        print(\"Downloading global weights for aggregation...\")\n",
    "        response = requests.get(\n",
    "            url=f\"{server}/app/model/download_gloabl_weights?model_id={model_id}\",\n",
    "            verify=\"./workspace/sandbox_common/service_cert.pem\",\n",
    "            cert=(user_cert, user_key)\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            print(\"Global weights downloaded successfully.\")\n",
    "            response_data = json.loads(response.text)\n",
    "            global_model_value = response_data.get(\"global_model\")\n",
    "            if global_model_value:\n",
    "                unflattened_weights =deserialize_weights(global_model_value, local_model)\n",
    "                print(\"Global model weights downloaded and unflattened successfully.\")\n",
    "                return unflattened_weights\n",
    "            else:\n",
    "                print(\"Global model data not found in response.\")\n",
    "        else:\n",
    "            print(f\"Failed to download global weights. Status code: {response.status_code}\")\n",
    "\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while downloading global model weights: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def aggregate_weight(client_weights_list):\n",
    "    if client_weights_list:\n",
    "        total_weights = sum(client_weights_list, [])\n",
    "        aggregated_weights = [weight / len(client_weights_list) for weight in total_weights]\n",
    "        return aggregated_weights\n",
    "    else:\n",
    "        return []\n",
    "def create_model():\n",
    "    print(\"Initializing the global model...\")\n",
    "    model = Sequential([\n",
    "        Conv2D(64, kernel_size=3, activation='relu', input_shape=(28, 28, 1)),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(32, kernel_size=3, activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def aggregate_weights(model_id,round_no, user_cert, user_key):\n",
    "    print(\"Aggregating weights for model:\", model_id)\n",
    "  \n",
    "    response = requests.put(\n",
    "        url=f\"{server}/app/model/aggregate_weights_local?model_id={model_id}&&round_no={round_no}\",\n",
    "        verify=\"./workspace/sandbox_common/service_cert.pem\",\n",
    "        cert=(user_cert, user_key)\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        print(\"Aggregation successful for model:\", model_id)\n",
    "    else:\n",
    "        raise Exception(f\"Failed to aggregate weights. Status code: {response.status_code}\")\n",
    "def train_model(model, X_train, y_train, X_test, y_test, user_id, round_no, epochs=2):\n",
    "    print(f\"Training model for User {user_id}, Round {round_no}...\")\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs)\n",
    "    return history.history['loss']\n",
    "\n",
    "def serialize_model(model):\n",
    "    print(\"Serializing the model...\")\n",
    "    model.save('temp_model.h5')\n",
    "    with open('temp_model.h5', 'rb') as file:\n",
    "        return base64.b64encode(file.read()).decode('utf-8')\n",
    "    \n",
    "\n",
    "def upload_initial_model(model_base64, user_cert, user_key):\n",
    "    print(\"Uploading initial global model...\")\n",
    "    payload = {\n",
    "      \"global_model\": {\n",
    "        \"model_name\": \"CNNModel\",\n",
    "        \"model_data\": model_base64\n",
    "      }\n",
    "    }\n",
    "    response = requests.post(\n",
    "        url=f\"{server}/app/model/intial_model\",\n",
    "        verify=\"./workspace/sandbox_common/service_cert.pem\",\n",
    "        cert=(user_cert, user_key),\n",
    "        json=payload\n",
    "    )\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        model_data = response.json()\n",
    "        model_id = model_data.get(\"model_id\")\n",
    "        model_name = model_data.get(\"model_name\")\n",
    "        print(f\"Initial global model '{model_name}' (ID: {model_id}) uploaded successfully.\")\n",
    "        return model_id\n",
    "    else:\n",
    "        print(f\"Failed to upload initial model. Status code: {response.status_code}\")\n",
    "        return None\n",
    "def flatten_weights(model):\n",
    "    \"\"\"Flatten the model weights\"\"\"\n",
    "    flat_weights = []\n",
    "    for layer in model.layers:\n",
    "        weights = layer.get_weights()\n",
    "        if weights:\n",
    "            flat_weights.append(weights[0].flatten())\n",
    "    return np.concatenate(flat_weights)\n",
    "def deserialize_weights(serialized_weights, model):\n",
    "    \"\"\"Deserialize the flattened weights and return them without updating the model\"\"\"\n",
    "    flat_weights = np.array(serialized_weights)\n",
    "    unflattened_weights = unflatten_weights(model, flat_weights)\n",
    "    return unflattened_weights\n",
    "\n",
    "\n",
    "def unflatten_weights(model, flat_weights):\n",
    "    unflattened_weights = []\n",
    "    index = 0\n",
    "\n",
    "    for layer in model.layers:\n",
    "        layer_weights = layer.get_weights()\n",
    "        if layer_weights:\n",
    "            weights_shape = layer_weights[0].shape\n",
    "            layer_weights_unflattened = flat_weights[index:index + np.prod(weights_shape)].reshape(weights_shape)\n",
    "            unflattened_weights.append(layer_weights_unflattened)\n",
    "            index += np.prod(weights_shape)\n",
    "\n",
    "    # Convert the list of arrays to a NumPy array\n",
    "    unflattened_weights = [np.array(arr) for arr in unflattened_weights]\n",
    "    return unflattened_weights\n",
    "\n",
    "\n",
    "\n",
    "# def serialize_weights(model):\n",
    "#     \"\"\"Serialize the model weights by flattening them\"\"\"\n",
    "#     # flat_weights = flatten_weights(model)\n",
    "#     local_model_weights = model.get_weights()\n",
    "#     # Serialize the weights using numpy's tolist() method\n",
    "\n",
    "    \n",
    "#     serialized_weights = json.dumps(local_model_weights)\n",
    "#     return serialized_weights\n",
    "def serialize_weights(model):\n",
    "    \"\"\"Serialize the model weights by flattening them\"\"\"\n",
    "    # Get the model weights\n",
    "    local_model_weights = model.get_weights()\n",
    "    \n",
    "    # Convert numpy arrays to lists\n",
    "    serialized_weights = [arr.tolist() for arr in local_model_weights]\n",
    "    \n",
    "    # Serialize the weights using json.dumps\n",
    "    serialized_weights_json = json.dumps(serialized_weights)\n",
    "    \n",
    "    return serialized_weights_json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def upload_model_weights(model_weights_base64, user_cert, user_key, round_no, model_id=None):\n",
    "    \"\"\" Upload only the model weights \"\"\"\n",
    "    print(f\"Uploading model weights for Round {round_no}...\")\n",
    "    payload = {\n",
    "        \"model_id\": model_id,\n",
    "        \"weights_json\": model_weights_base64,\n",
    "        \"round_no\": round_no\n",
    "    }\n",
    "    response = requests.post(\n",
    "        url=f\"{server}/app/model/upload/local_model_weights\",\n",
    "        verify=\"./workspace/sandbox_common/service_cert.pem\",\n",
    "        cert=(user_cert, user_key),\n",
    "        json=payload\n",
    "    )\n",
    "    print(response.text)\n",
    "    if response:\n",
    "        print(f\"Model weights uploaded successfully for Round {round_no}.\")\n",
    "    else:\n",
    "        raise Exception(f\"Failed to upload model weights. Status code: {response.status_code}\")\n",
    "\n",
    "def download_model(user_cert, user_key, user_id, round_no, max_retries=5, model_id=None):\n",
    "    attempts = 0\n",
    "    while attempts < max_retries:\n",
    "        print(f\"Attempting to download model for User {user_id}, Round {round_no}, Attempt {attempts + 1}...\")\n",
    "        response = requests.get(\n",
    "            url=f\"{server}/app/model/download/global?model_id={model_id}\",\n",
    "            verify=\"./workspace/sandbox_common/service_cert.pem\",\n",
    "            cert=(user_cert, user_key)\n",
    "        )\n",
    "  \n",
    "\n",
    "        if response.status_code == 200:\n",
    "            model_data = response.json().get(\"model_details\", {})\n",
    "            model_base64 = model_data\n",
    "            \n",
    "            if model_base64:\n",
    "                with open('temp_model.h5', 'wb') as file:\n",
    "                    file.write(base64.b64decode(model_base64))\n",
    "                return load_model('temp_model.h5')\n",
    "            else:\n",
    "                print(\"Model data not found in response, retrying...\")\n",
    "        else:\n",
    "            print(f\"Failed to download model. Status code: {response.status_code}, retrying...\")\n",
    "        \n",
    "        time.sleep(2)  # Delay before retrying\n",
    "        attempts += 1\n",
    "    \n",
    "    raise Exception(\"Failed to download model after maximum retries.\")\n",
    "def delete_temp_model_file():\n",
    "    temp_model_path = 'temp_model.h5'\n",
    "    if os.path.exists(temp_model_path):\n",
    "        os.remove(temp_model_path)\n",
    "        print(f\"Deleted temporary model file: {temp_model_path}\")\n",
    "    else:\n",
    "        print(f\"No temporary model file found at: {temp_model_path}\")\n",
    "\n",
    "def plot_loss(user_losses):\n",
    "    for user_id, losses in user_losses.items():\n",
    "        plt.plot(losses, label=f'User {user_id}')\n",
    "    plt.title('Model Loss per Training Round')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Round')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "def update_local_model(local_model, global_model_weights):\n",
    "    if global_model_weights:\n",
    "        global_model_weights = [np.array(w) for w in global_model_weights]\n",
    "        \n",
    "        # Get the layers in the local model\n",
    "        local_layers = local_model.layers\n",
    "        print(f\"Local model layers: {local_layers}\")\n",
    "        print(f\"Global model shape: {len(global_model_weights)}\")\n",
    "        # Set weights for each layer individually\n",
    "        for i in range(len(local_layers)):\n",
    "            if i < len(global_model_weights):\n",
    "                if isinstance(local_layers[i], Conv2D):\n",
    "                    # Check if it's a Conv2D layer\n",
    "                    weights_shape = local_layers[i].get_weights()[0].shape\n",
    "                    provided_weights_shape = global_model_weights[i][0].shape\n",
    "                    print(f\"Layer {local_layers[i].name} - Expected shape: {weights_shape}, Provided shape: {provided_weights_shape}\")\n",
    "                    \n",
    "                    if weights_shape == provided_weights_shape:\n",
    "                        local_layers[i].set_weights([global_model_weights[i][0], global_model_weights[i][1]])\n",
    "                    else:\n",
    "                        try:\n",
    "                            reshaped_kernel_weights = np.reshape(global_model_weights[i][0], weights_shape)\n",
    "                            local_layers[i].set_weights([reshaped_kernel_weights, global_model_weights[i][1]])\n",
    "                        except ValueError as e:\n",
    "                            print(f\"Error reshaping weights for layer {local_layers[i].name}: {e}\")\n",
    "                elif isinstance(local_layers[i], Dense):\n",
    "                    # Check if it's a Dense layer\n",
    "                    local_layers[i].set_weights(global_model_weights[i])\n",
    "                # Add similar checks for other layer types if needed\n",
    "                else:\n",
    "                    # Skip setting weights for layers without trainable weights\n",
    "                    print(f\"Skipped setting weights for layer {local_layers[i].name}: No trainable weights\")\n",
    "            else:\n",
    "                print(f\"Warning: No weights provided for layer {local_layers[i].name}\")\n",
    "\n",
    "        print(\"Global model weights updated for the local model.\")\n",
    "    else:\n",
    "        print(\"No global model weights provided.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing MNIST dataset...\n",
      "No temporary model file found at: temp_model.h5\n",
      "Splitting dataset for two users...\n",
      "Initializing the global model...\n",
      "Serializing the model...\n",
      "Uploading initial global model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/CCF_FL_Block/.venv/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial global model 'CNNModel' (ID: 7) uploaded successfully.\n",
      "Attempting to download model for User 0, Round 0, Attempt 1...\n",
      "Training model for User 0, Round 1...\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 19:40:17.712772: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 23520000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/938 [..............................] - ETA: 32s - loss: 28.1357 - accuracy: 0.1562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 19:40:18.405109: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 23975424 exceeds 10% of free system memory.\n",
      "2024-05-19 19:40:18.405244: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 23975424 exceeds 10% of free system memory.\n",
      "2024-05-19 19:40:18.444173: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 23975424 exceeds 10% of free system memory.\n",
      "2024-05-19 19:40:18.444267: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 23975424 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 22s 23ms/step - loss: 0.6120 - accuracy: 0.9003 - val_loss: 0.1344 - val_accuracy: 0.9609\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 21s 22ms/step - loss: 0.1083 - accuracy: 0.9677 - val_loss: 0.1061 - val_accuracy: 0.9694\n",
      "Uploading model weights for Round 1...\n",
      "{\"message\":\"Model weights received\"}\n",
      "Model weights uploaded successfully for Round 1.\n",
      "Training model for User 1, Round 1...\n",
      "Epoch 1/2\n",
      "938/938 [==============================] - 21s 23ms/step - loss: 0.0975 - accuracy: 0.9714 - val_loss: 0.0708 - val_accuracy: 0.9777\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 21s 22ms/step - loss: 0.0649 - accuracy: 0.9793 - val_loss: 0.0675 - val_accuracy: 0.9795\n",
      "Uploading model weights for Round 1...\n",
      "{\"message\":\"Model weights received\"}\n",
      "Model weights uploaded successfully for Round 1.\n",
      "Aggregating weights for model: 7\n",
      "Aggregation successful for model: 7\n",
      "Downloading global weights for aggregation...\n",
      "Failed to download global weights. Status code: 404\n",
      "Training model for User 0, Round 2...\n",
      "Epoch 1/2\n",
      "500/938 [==============>...............] - ETA: 9s - loss: 0.0738 - accuracy: 0.9786"
     ]
    }
   ],
   "source": [
    "# Load and preprocess MNIST dataset\n",
    "print(\"Loading and preprocessing MNIST dataset...\")\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "delete_temp_model_file()\n",
    "# Split the dataset for two users\n",
    "print(\"Splitting dataset for two users...\")\n",
    "split_index = int(len(X_train) / 2)\n",
    "X_train_user0, X_train_user1 = np.split(X_train, [split_index])\n",
    "y_train_user0, y_train_user1 = np.split(y_train, [split_index])\n",
    "\n",
    "# Initialize and train the global model (User 0)\n",
    "global_model = create_model()\n",
    "# train_model(global_model, X_train_user0, y_train_user0, X_test, y_test, user_id=0, round_no=0)\n",
    "\n",
    "\n",
    "# Serialize and upload initial global model\n",
    "model_base64 = serialize_model(global_model)\n",
    "\n",
    "initial_model_id = upload_initial_model(model_base64, \"./workspace/sandbox_common/user0_cert.pem\", \"./workspace/sandbox_common/user0_privk.pem\")\n",
    "\n",
    "\n",
    "if initial_model_id is None:\n",
    "    raise Exception(\"Failed to upload the initial global model. Stopping the process.\")\n",
    "num_rounds = 4  # Number of training rounds\n",
    "user_losses = {0: [], 1: []}  # To track losses for each user\n",
    "\n",
    "# Download the latest global model once for each client\n",
    "global_model = download_model(\n",
    "    \"./workspace/sandbox_common/user0_cert.pem\",\n",
    "    \"./workspace/sandbox_common/user0_privk.pem\",\n",
    "    user_id=0, round_no=0, model_id=initial_model_id\n",
    ")\n",
    "\n",
    "# Initialize the local models for both users\n",
    "local_model_user0 = global_model\n",
    "local_model_user1 = global_model\n",
    "\n",
    "for round_no in range(1, num_rounds + 1):\n",
    "    for user_id in range(2):  # Two users: 0 and 1\n",
    "        # Train the local model on the user's data\n",
    "        X_train_user = X_train_user0 if user_id == 0 else X_train_user1\n",
    "        y_train_user = y_train_user0 if user_id == 0 else y_train_user1\n",
    "        loss = train_model(local_model_user0 if user_id == 0 else local_model_user1,\n",
    "                           X_train_user, y_train_user, X_test, y_test, user_id, round_no)\n",
    "        user_losses[user_id].extend(loss)\n",
    "\n",
    "        # Serialize and upload the updated model weights for the current round\n",
    "        local_serialize_weights = serialize_weights(local_model_user0 if user_id == 0 else local_model_user1)\n",
    "        # Upload model weights only if they are not empty\n",
    "\n",
    "        if local_serialize_weights:\n",
    "            upload_model_weights(local_serialize_weights, f\"./workspace/sandbox_common/user{user_id}_cert.pem\",\n",
    "                                 f\"./workspace/sandbox_common/user{user_id}_privk.pem\", round_no, model_id=initial_model_id)\n",
    "        else:\n",
    "            print(\"Model weights are empty, skipping upload...\")\n",
    "\n",
    "    # Aggregate weights after each user's training round\n",
    "    aggregate_weights(initial_model_id,round_no, \"./workspace/sandbox_common/member0_cert.pem\",\n",
    "                      \"./workspace/sandbox_common/member0_privk.pem\")\n",
    "\n",
    "    # Download global model weights after aggregation and update local models\n",
    "    global_model_weights = download_global_model_weights(\n",
    "        \"./workspace/sandbox_common/user0_cert.pem\", \"./workspace/sandbox_common/user0_privk.pem\",\n",
    "        model_id=initial_model_id,\n",
    "        local_model=local_model_user0\n",
    "    )\n",
    "\n",
    "print(\"Federated Learning Process Completed.\")\n",
    "\n",
    "# Plot loss graphs for each user\n",
    "plot_loss(user_losses)\n",
    "delete_temp_model_file()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
