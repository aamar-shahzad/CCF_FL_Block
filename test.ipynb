{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "\n",
      "Response Headers:\n",
      "content-length: 176\n",
      "content-type: application/json\n",
      "\n",
      "Response Body:\n",
      "{\n",
      "    \"metrics\": [\n",
      "        {\n",
      "            \"calls\": 1,\n",
      "            \"errors\": 0,\n",
      "            \"failures\": 0,\n",
      "            \"method\": \"GET\",\n",
      "            \"path\": \"api/metrics\",\n",
      "            \"retries\": 0\n",
      "        },\n",
      "        {\n",
      "            \"calls\": 2,\n",
      "            \"errors\": 0,\n",
      "            \"failures\": 0,\n",
      "            \"method\": \"GET\",\n",
      "            \"path\": \"commit\",\n",
      "            \"retries\": 0\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import base64\n",
    "import hashlib\n",
    "from urllib.parse import urlparse\n",
    "import subprocess\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "RSA_SIZE = 2048\n",
    "\n",
    "\n",
    "DEFAULT_CURVE = \"secp384r1\"\n",
    "FAST_CURVE = \"secp256r1\"\n",
    "SUPPORTED_CURVES = [DEFAULT_CURVE, FAST_CURVE]\n",
    "\n",
    "DIGEST_SHA384 = \"sha384\"\n",
    "DIGEST_SHA256 = \"sha256\"\n",
    "\n",
    "RSA_SIZE = 2048\n",
    "# CCF network node\n",
    "server=\"https://127.0.0.1:8000\"\n",
    "\n",
    "num_users = 4\n",
    "# Getting Network metrices\n",
    "url = server + \"/app/api/metrics\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, verify='./workspace/sandbox_common/service_cert.pem')\n",
    "\n",
    "    print(\"Status Code:\", response.status_code)\n",
    "    print(\"\\nResponse Headers:\")\n",
    "    for header, value in response.headers.items():\n",
    "        print(f\"{header}: {value}\")\n",
    "\n",
    "    print(\"\\nResponse Body:\")\n",
    "    try:\n",
    "        # Attempt to parse JSON and print it in an indented format\n",
    "        response_json = response.json()\n",
    "        print(json.dumps(response_json, indent=4))\n",
    "    except ValueError:\n",
    "        # If response is not JSON, print as plain text\n",
    "        print(response.text)\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"Error making request:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ryptography (/workspaces/ccf-app-template/.venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: matplotlib in ./.venv/lib/python3.8/site-packages (3.7.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.8/site-packages (from matplotlib) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.20 in ./.venv/lib/python3.8/site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.8/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib) (6.1.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ryptography (/workspaces/ccf-app-template/.venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ryptography (/workspaces/ccf-app-template/.venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tensorflow in ./.venv/lib/python3.8/site-packages (2.13.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./.venv/lib/python3.8/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./.venv/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in ./.venv/lib/python3.8/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in ./.venv/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./.venv/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.venv/lib/python3.8/site-packages (from tensorflow) (1.60.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in ./.venv/lib/python3.8/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in ./.venv/lib/python3.8/site-packages (from tensorflow) (2.13.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./.venv/lib/python3.8/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in ./.venv/lib/python3.8/site-packages (from tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./.venv/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.8/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./.venv/lib/python3.8/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.8/site-packages (from tensorflow) (44.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in ./.venv/lib/python3.8/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in ./.venv/lib/python3.8/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./.venv/lib/python3.8/site-packages (from tensorflow) (2.4.0)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow)\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./.venv/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./.venv/lib/python3.8/site-packages (from tensorflow) (0.34.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./.venv/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./.venv/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in ./.venv/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.venv/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./.venv/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.venv/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.venv/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.venv/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.venv/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.venv/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./.venv/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./.venv/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (7.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.4)\n",
      "Requirement already satisfied: zipp>=0.5 in ./.venv/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (3.17.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in ./.venv/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./.venv/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ryptography (/workspaces/ccf-app-template/.venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: typing-extensions\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "azure-core 1.29.7 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed typing-extensions-4.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-28 23:20:33.698552: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-28 23:20:33.751680: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-28 23:20:33.753260: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-28 23:20:34.594783: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import base64\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "server = \"https://127.0.0.1:8000\"  # Replace with your server URL\n",
    "\n",
    "\n",
    "\n",
    "def download_global_model_weights(user_cert, user_key, model_id):\n",
    "    try:\n",
    "        print(\"Downloading client weights for aggregation...\")\n",
    "        response = requests.get(\n",
    "            url=f\"{server}/app/model/download_gloabl_weights?model_id={model_id}\",\n",
    "            verify=\"./workspace/sandbox_common/service_cert.pem\",\n",
    "            cert=(user_cert, user_key)\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            print(\"Client weights downloaded successfully.\")\n",
    "            response_data = json.loads(response.text)\n",
    "            global_model_value = response_data.get(\"global_model\")\n",
    "            return global_model_value\n",
    "    \n",
    "\n",
    "            # # Access the value associated with the \"gloablmodel\" key\n",
    "            # global_model_value = response_data.get(\"global_model\")\n",
    "            # model_id = global_model_value.get(\"model_id\")\n",
    "            # print(f\"Global model ID: {model_id}\")\n",
    "            # return global_model_value\n",
    "            \n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to download client weights. Status code: {response.status_code}\")\n",
    "\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while downloading global model weights: {e}\")\n",
    "        return None\n",
    "def aggregate_weight(client_weights_list):\n",
    "    if client_weights_list:\n",
    "        total_weights = sum(client_weights_list, [])\n",
    "        aggregated_weights = [weight / len(client_weights_list) for weight in total_weights]\n",
    "        return aggregated_weights\n",
    "    else:\n",
    "        return []\n",
    "def create_model():\n",
    "    print(\"Initializing the global model...\")\n",
    "    model = Sequential([\n",
    "        Conv2D(64, kernel_size=3, activation='relu', input_shape=(28, 28, 1)),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(32, kernel_size=3, activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def aggregate_weights(model_id,round_no, user_cert, user_key):\n",
    "    print(\"Aggregating weights for model:\", model_id)\n",
    "  \n",
    "    response = requests.put(\n",
    "        url=f\"{server}/app/model/aggregate_weights_local?model_id={model_id}&&round_no={round_no}\",\n",
    "        verify=\"./workspace/sandbox_common/service_cert.pem\",\n",
    "        cert=(user_cert, user_key)\n",
    "    )\n",
    "    print(response.text)\n",
    "    if response.status_code == 200:\n",
    "        print(\"Aggregation successful for model:\", model_id)\n",
    "    else:\n",
    "        raise Exception(f\"Failed to aggregate weights. Status code: {response.status_code}\")\n",
    "def train_model(model, X_train, y_train, X_test, y_test, user_id, round_no, epochs=2):\n",
    "    print(f\"Training model for User {user_id}, Round {round_no}...\")\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs)\n",
    "    return history.history['loss']\n",
    "\n",
    "def serialize_model(model):\n",
    "    print(\"Serializing the model...\")\n",
    "    model.save('temp_model.h5')\n",
    "    with open('temp_model.h5', 'rb') as file:\n",
    "        return base64.b64encode(file.read()).decode('utf-8')\n",
    "    \n",
    "\n",
    "def upload_initial_model(model_base64, user_cert, user_key):\n",
    "    print(\"Uploading initial global model...\")\n",
    "    payload = {\n",
    "      \"global_model\": {\n",
    "        \"model_name\": \"CNNModel\",\n",
    "        \"model_data\": model_base64\n",
    "      }\n",
    "    }\n",
    "    response = requests.post(\n",
    "        url=f\"{server}/app/model/intial_model\",\n",
    "        verify=\"./workspace/sandbox_common/service_cert.pem\",\n",
    "        cert=(user_cert, user_key),\n",
    "        json=payload\n",
    "    )\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        model_data = response.json()\n",
    "        model_id = model_data.get(\"model_id\")\n",
    "        model_name = model_data.get(\"model_name\")\n",
    "        print(f\"Initial global model '{model_name}' (ID: {model_id}) uploaded successfully.\")\n",
    "        return model_id\n",
    "    else:\n",
    "        print(f\"Failed to upload initial model. Status code: {response.status_code}\")\n",
    "        return None\n",
    "def deserialize_weights(serialized_weights):\n",
    "    \"\"\" Deserialize serialized weights and set them to the model \"\"\"\n",
    "    print(\"Deserializing model weights...\")\n",
    "    \n",
    "    # Parse the JSON-formatted string to obtain serialized weights\n",
    "    deserialized_weights = json.loads(serialized_weights)\n",
    "    \n",
    "    # Convert the serialized weights back to numpy arrays\n",
    "    value = [np.array(w) if isinstance(w, list) else w for w in deserialized_weights]\n",
    "    \n",
    "    # Set the deserialized weights to the model\n",
    "    print(\"Model weights deserialized and set successfully.\")\n",
    "    return value\n",
    "    \n",
    "\n",
    "def serialize_weights(model):\n",
    "    \"\"\" Serialize only the weights of the model \"\"\"\n",
    "    print(\"Serializing model weights...\")\n",
    "    weights = model.get_weights()  # Get the model weights as a list of numpy arrays\n",
    "    #  print the shape of the weights\n",
    "    # print(\"Weights shape: before serialization\")\n",
    "    # for w in weights:\n",
    "    #     print(w.shape)\n",
    "    # Convert the weights to a JSON-serializable format\n",
    "    serialized_weights = [w.tolist() if isinstance(w, np.ndarray) else w for w in weights]\n",
    "    # print the shape of the serialized weights\n",
    "    print(\"Weights shape: after serialization\")\n",
    "    # for w in serialized_weights:\n",
    "    #     print(len(w))\n",
    "    \n",
    "    return json.dumps(serialized_weights)\n",
    "\n",
    "\n",
    "def upload_model_weights(model_weights_base64, user_cert, user_key, round_no, model_id=None):\n",
    "    \"\"\" Upload only the model weights \"\"\"\n",
    "    print(f\"Uploading model weights for Round {round_no}...\")\n",
    "    payload = {\n",
    "        \"model_id\": model_id,\n",
    "        \"weights_json\": model_weights_base64,\n",
    "        \"round_no\": round_no\n",
    "    }\n",
    "    response = requests.post(\n",
    "        url=f\"{server}/app/model/upload/local_model_weights\",\n",
    "        verify=\"./workspace/sandbox_common/service_cert.pem\",\n",
    "        cert=(user_cert, user_key),\n",
    "        json=payload\n",
    "    )\n",
    "    print(response.text)\n",
    "    if response:\n",
    "        print(f\"Model weights uploaded successfully for Round {round_no}.\")\n",
    "    else:\n",
    "        raise Exception(f\"Failed to upload model weights. Status code: {response.status_code}\")\n",
    "\n",
    "def download_model(user_cert, user_key, user_id, round_no, max_retries=5, model_id=None):\n",
    "    attempts = 0\n",
    "    while attempts < max_retries:\n",
    "        print(f\"Attempting to download model for User {user_id}, Round {round_no}, Attempt {attempts + 1}...\")\n",
    "        response = requests.get(\n",
    "            url=f\"{server}/app/model/download/global?model_id={model_id}\",\n",
    "            verify=\"./workspace/sandbox_common/service_cert.pem\",\n",
    "            cert=(user_cert, user_key)\n",
    "        )\n",
    "  \n",
    "\n",
    "        if response.status_code == 200:\n",
    "            model_data = response.json().get(\"model_details\", {})\n",
    "            model_base64 = model_data\n",
    "            \n",
    "            if model_base64:\n",
    "                with open('temp_model.h5', 'wb') as file:\n",
    "                    file.write(base64.b64decode(model_base64))\n",
    "                return load_model('temp_model.h5')\n",
    "            else:\n",
    "                print(\"Model data not found in response, retrying...\")\n",
    "        else:\n",
    "            print(f\"Failed to download model. Status code: {response.status_code}, retrying...\")\n",
    "        \n",
    "        time.sleep(2)  # Delay before retrying\n",
    "        attempts += 1\n",
    "    \n",
    "    raise Exception(\"Failed to download model after maximum retries.\")\n",
    "\n",
    "def plot_loss(user_losses):\n",
    "    for user_id, losses in user_losses.items():\n",
    "        plt.plot(losses, label=f'User {user_id}')\n",
    "    plt.title('Model Loss per Training Round')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Round')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing MNIST dataset...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n",
      "Splitting dataset for two users...\n",
      "Initializing the global model...\n",
      "Training model for User 0, Round 0...\n",
      "Epoch 1/2\n",
      "938/938 [==============================] - 14s 14ms/step - loss: 0.6313 - accuracy: 0.8975 - val_loss: 0.1492 - val_accuracy: 0.9551\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 13s 14ms/step - loss: 0.1169 - accuracy: 0.9661 - val_loss: 0.0995 - val_accuracy: 0.9710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6313133239746094, 0.1168571338057518]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preprocess MNIST dataset\n",
    "print(\"Loading and preprocessing MNIST dataset...\")\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Split the dataset for two users\n",
    "print(\"Splitting dataset for two users...\")\n",
    "split_index = int(len(X_train) / 2)\n",
    "X_train_user0, X_train_user1 = np.split(X_train, [split_index])\n",
    "y_train_user0, y_train_user1 = np.split(y_train, [split_index])\n",
    "\n",
    "# Initialize and train the global model (User 0)\n",
    "global_model = create_model()\n",
    "train_model(global_model, X_train_user0, y_train_user0, X_test, y_test, user_id=0, round_no=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serializing the model...\n",
      "Uploading initial global model...\n",
      "Initial global model 'CNNModel' (ID: 0) uploaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/ccf-app-template/.venv/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Serialize and upload initial global model\n",
    "model_base64 = serialize_model(global_model)\n",
    "\n",
    "initial_model_id = upload_initial_model(model_base64, \"./workspace/sandbox_common/user0_cert.pem\", \"./workspace/sandbox_common/user0_privk.pem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download model for User 0, Round 0, Attempt 1...\n",
      "Training model for User 0, Round 1...\n",
      "Epoch 1/2\n",
      "938/938 [==============================] - 13s 13ms/step - loss: 0.0826 - accuracy: 0.9760 - val_loss: 0.0933 - val_accuracy: 0.9714\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 13s 14ms/step - loss: 0.0672 - accuracy: 0.9801 - val_loss: 0.0848 - val_accuracy: 0.9757\n",
      "Serializing model weights...\n",
      "Weights shape: after serialization\n",
      "Uploading model weights for Round 1...\n",
      "{\"message\":\"Model weights received\"}\n",
      "Model weights uploaded successfully for Round 1.\n",
      "Training model for User 1, Round 1...\n",
      "Epoch 1/2\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.0842 - accuracy: 0.9748 - val_loss: 0.0672 - val_accuracy: 0.9786\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.0554 - accuracy: 0.9832 - val_loss: 0.0657 - val_accuracy: 0.9823\n",
      "Serializing model weights...\n",
      "Weights shape: after serialization\n",
      "Uploading model weights for Round 1...\n",
      "{\"message\":\"Model weights received\"}\n",
      "Model weights uploaded successfully for Round 1.\n",
      "Aggregating weights for model: 0\n",
      "{\"global_model\":[],\"message\":\"Global model retrieved successfully\",\"model_id\":0}\n",
      "Aggregation successful for model: 0\n",
      "Downloading client weights for aggregation...\n",
      "Failed to download client weights. Status code: 404\n",
      "Training model for User 0, Round 2...\n",
      "Epoch 1/2\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.0666 - accuracy: 0.9804 - val_loss: 0.0553 - val_accuracy: 0.9818\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.0452 - accuracy: 0.9862 - val_loss: 0.0740 - val_accuracy: 0.9807\n",
      "Serializing model weights...\n",
      "Weights shape: after serialization\n",
      "Uploading model weights for Round 2...\n",
      "{\"message\":\"Model weights received\"}\n",
      "Model weights uploaded successfully for Round 2.\n",
      "Training model for User 1, Round 2...\n",
      "Epoch 1/2\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.0629 - accuracy: 0.9820 - val_loss: 0.0513 - val_accuracy: 0.9840\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.0379 - accuracy: 0.9879 - val_loss: 0.0746 - val_accuracy: 0.9801\n",
      "Serializing model weights...\n",
      "Weights shape: after serialization\n",
      "Uploading model weights for Round 2...\n",
      "{\"message\":\"Model weights received\"}\n",
      "Model weights uploaded successfully for Round 2.\n",
      "Aggregating weights for model: 0\n",
      "{\"global_model\":[],\"message\":\"Global model retrieved successfully\",\"model_id\":0}\n",
      "Aggregation successful for model: 0\n",
      "Downloading client weights for aggregation...\n",
      "Failed to download client weights. Status code: 404\n",
      "Training model for User 0, Round 3...\n",
      "Epoch 1/2\n",
      "777/938 [=======================>......] - ETA: 1s - loss: 0.0502 - accuracy: 0.9844"
     ]
    }
   ],
   "source": [
    "\n",
    "if initial_model_id is None:\n",
    "    raise Exception(\"Failed to upload the initial global model. Stopping the process.\")\n",
    "\n",
    "# Wait for some time after initial upload\n",
    "# time.sleep(1)  # Wait for 10 seconds\n",
    "\n",
    "# Federated Learning Process\n",
    "# Federated Learning Process\n",
    "num_rounds = 4  # Number of training rounds\n",
    "user_losses = {0: [], 1: []}  # To track losses for each user\n",
    "\n",
    "# Download the latest global model once for each client\n",
    "global_model = download_model(\n",
    "    \"./workspace/sandbox_common/user0_cert.pem\",\n",
    "    \"./workspace/sandbox_common/user0_privk.pem\",\n",
    "    user_id=0, round_no=0, model_id=initial_model_id\n",
    ")\n",
    "\n",
    "# Initialize the local models for both users\n",
    "local_model_user0 = global_model\n",
    "local_model_user1 = global_model\n",
    "\n",
    "for round_no in range(1, num_rounds + 1):\n",
    "    for user_id in range(2):  # Two users: 0 and 1\n",
    "        # Train the local model on the user's data\n",
    "        X_train_user = X_train_user0 if user_id == 0 else X_train_user1\n",
    "        y_train_user = y_train_user0 if user_id == 0 else y_train_user1\n",
    "        loss = train_model(local_model_user0 if user_id == 0 else local_model_user1,\n",
    "                           X_train_user, y_train_user, X_test, y_test, user_id, round_no)\n",
    "        user_losses[user_id].extend(loss)\n",
    "\n",
    "        # Serialize and upload the updated model weights for the current round\n",
    "        model_weights_base64 = serialize_weights(local_model_user0 if user_id == 0 else local_model_user1)\n",
    "        # Upload model weights only if they are not empty\n",
    "\n",
    "        if model_weights_base64:\n",
    "            upload_model_weights(model_weights_base64, f\"./workspace/sandbox_common/user{user_id}_cert.pem\",\n",
    "                                 f\"./workspace/sandbox_common/user{user_id}_privk.pem\", round_no, model_id=initial_model_id)\n",
    "        else:\n",
    "            print(\"Model weights are empty, skipping upload...\")\n",
    "\n",
    "    # Aggregate weights after each user's training round\n",
    "    aggregate_weights(initial_model_id,round_no, \"./workspace/sandbox_common/member0_cert.pem\",\n",
    "                      \"./workspace/sandbox_common/member0_privk.pem\")\n",
    "\n",
    "    # Download global model weights after aggregation and update local models\n",
    "    global_model_weights = download_global_model_weights(\n",
    "        \"./workspace/sandbox_common/user0_cert.pem\", \"./workspace/sandbox_common/user0_privk.pem\",\n",
    "        model_id=initial_model_id\n",
    "    )\n",
    "\n",
    "    # if global_model_weights:\n",
    "    #     # Update local models with global weights\n",
    "    #     local_model_user0.set_weights([np.array(w) for w in global_model_weights])\n",
    "    #     local_model_user1.set_weights([np.array(w) for w in global_model_weights])\n",
    "    #     print(\"Global model weights updated for both clients.\")\n",
    "\n",
    "print(\"Federated Learning Process Completed.\")\n",
    "\n",
    "# Plot loss graphs for each user\n",
    "plot_loss(user_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 2 column 1 (char 557311)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     serialized_weights \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Deserialize the weights\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m deserialized_weights \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserialized_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Assuming 'model' is your Keras model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Set the weights of the model\u001b[39;00m\n\u001b[1;32m     12\u001b[0m deserialize_weights\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/__init__.py:357\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    355\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    356\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 557311)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Read the serialized weights from the test.txt file\n",
    "with open('test.txt', 'r') as file:\n",
    "    serialized_weights = file.read()\n",
    "\n",
    "# Deserialize the weights\n",
    "deserialized_weights = json.loads(serialized_weights)\n",
    "\n",
    "# Assuming 'model' is your Keras model\n",
    "# Set the weights of the model\n",
    "deserialize_weights\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
